    import torch
    import torch.nn as nn
    import torch.optim as optim
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Rectangle, Circle
    import random
    from collections import deque
    import os
    import time

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º: {device}")

    class FastPatrolNet(nn.Module):
        def __init__(self):
            super(FastPatrolNet, self).__init__()
            self.net = nn.Sequential(
                nn.Conv2d(4, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(32, 64, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, kernel_size=3, padding=1),
                nn.ReLU(),
                nn.Flatten(),
                nn.Linear(32 * 10 * 10, 256),
                nn.ReLU(),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Linear(128, 4)
            )
        
        def forward(self, x):
            return self.net(x)

    class FixedPatrolEnv:
        def __init__(self):
            self.size = 10
            self.max_steps = 100
            self.walls = {(2,2),(2,3),(3,2),(3,3),
                        (6,6),(6,7),(7,6),(7,7),
                        (2,7),(3,7),(7,2),(7,3)}
            self.risk_zones = [(1,1),(1,8),(8,1),(8,8),(5,5)]
            self.reset()
        
        def reset(self):
            self.pos = [5, 5]
            self.targets = []
            self.steps = 0
            self.caught = 0
            self.missed = 0
            self.visit_count = np.zeros((10, 10), dtype=np.float32)
            self.spawn(force=True)
            return self.get_state()
        
        def get_state(self):
            state = np.zeros((4, 10, 10), dtype=np.float32)
            state[0, self.pos[0], self.pos[1]] = 1.0
            for (y, x) in self.walls:
                state[1, y, x] = 1.0
            for t in self.targets:
                state[1, t[0], t[1]] = -1.0
            max_visits = np.max(self.visit_count) + 1
            state[2] = self.visit_count / max_visits
            state[3] = 1.0 - state[2]
            return state
        
        def spawn(self, force=False):
            if len(self.targets) >= 1:
                return
            if not force and random.random() > 0.1:
                return
            point = random.choice(self.risk_zones)
            y = max(0, min(9, point[0] + random.randint(-1, 1)))
            x = max(0, min(9, point[1] + random.randint(-1, 1)))
            if (y, x) not in self.walls and [y, x] != self.pos:
                self.targets.append([y, x, 20])
        
        def step(self, action):
            reward = 0
            old_pos = self.pos.copy()
            y, x = self.pos
            
            if action == 0: y -= 1
            elif action == 1: y += 1
            elif action == 2: x -= 1
            elif action == 3: x += 1
            
            hit_wall = False
            if (y, x) in self.walls or not (0 <= y < 10 and 0 <= x < 10):
                hit_wall = True
                y, x = old_pos
                reward -= 0.5
            
            self.pos = [y, x]
            self.steps += 1
            self.visit_count[y, x] += 1
            
            # –ù–∞–≥—Ä–∞–¥—ã
            reward -= 0.02
            
            visits = self.visit_count[y, x]
            if visits == 1:
                reward += 2.0
            elif visits <= 2:
                reward += 0.5
            else:
                reward -= 0.5 * visits
            
            # –°–æ—Å–µ–¥–Ω–∏–µ –Ω–µ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã–µ
            unexplored = 0
            for dy in [-1, 0, 1]:
                for dx in [-1, 0, 1]:
                    if dy == 0 and dx == 0:
                        continue
                    ny, nx = y + dy, x + dx
                    if 0 <= ny < 10 and 0 <= nx < 10 and self.visit_count[ny, nx] == 0:
                        unexplored += 1
            reward += 0.3 * unexplored
            
            # –¶–µ–ª–∏
            caught = False
            new_targets = []
            for t in self.targets:
                if t[0] == y and t[1] == x:
                    reward += 1.0
                    self.caught += 1
                    caught = True
                else:
                    t[2] -= 1
                    if t[2] > 0:
                        new_targets.append(t)
                    else:
                        reward -= 1.0
                        self.missed += 1
            self.targets = new_targets
            
            if old_pos == self.pos and not hit_wall:
                reward -= 0.3
            
            self.spawn()
            done = (self.missed >= 2) or (self.steps >= self.max_steps)
            
            if done:
                coverage = np.sum(self.visit_count > 0) / 100.0
                reward += coverage * 5.0
            
            return self.get_state(), reward, done, caught

    class FastPatrolAgent:
        def __init__(self, load_model=True):
            self.net = FastPatrolNet().to(device)
            self.target_net = FastPatrolNet().to(device)
            self.target_net.load_state_dict(self.net.state_dict())
            self.target_net.eval()
            
            self.optimizer = optim.Adam(self.net.parameters(), lr=0.001)
            self.memory = deque(maxlen=10000)
            
            self.batch_size = 32
            self.gamma = 0.99
            self.epsilon = 1.0
            self.epsilon_min = 0.05
            self.epsilon_decay = 0.995
            self.update_every = 100
            
            self.steps_done = 0
            self.model_file = "fast_patrol_v2.pth"
            
            if load_model and os.path.exists(self.model_file):
                self.net.load_state_dict(torch.load(self.model_file))
                self.target_net.load_state_dict(self.net.state_dict())
                print("üß† –ó–∞–≥—Ä—É–∂–µ–Ω –æ–ø—ã—Ç")
        
        def select_action(self, state, training=True):
            if training and random.random() < self.epsilon:
                return random.randint(0, 3)
            with torch.no_grad():
                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
                q = self.net(state_t)
                return q.argmax().item()
        
        def store(self, s, a, r, s2, d):
            self.memory.append((s, a, r, s2, d))
        
        def learn(self):
            if len(self.memory) < self.batch_size:
                return 0
            batch = random.sample(self.memory, self.batch_size)
            s, a, r, s2, d = zip(*batch)
            s = torch.FloatTensor(np.array(s)).to(device)
            a = torch.LongTensor(a).to(device)
            r = torch.FloatTensor(r).to(device)
            s2 = torch.FloatTensor(np.array(s2)).to(device)
            d = torch.BoolTensor(d).to(device)
            
            q = self.net(s).gather(1, a.unsqueeze(1)).squeeze()
            with torch.no_grad():
                q2 = self.target_net(s2).max(1)[0]
                target = r + self.gamma * q2 * ~d
            
            loss = nn.functional.smooth_l1_loss(q, target)
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.net.parameters(), 1.0)
            self.optimizer.step()
            return loss.item()
        
        def update_target(self):
            self.target_net.load_state_dict(self.net.state_dict())
        
        def decay(self):
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
        
        def save(self):
            torch.save(self.net.state_dict(), self.model_file)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ, epsilon={self.epsilon:.3f}")

    def train_fast(episodes=2000):  # –£–í–ï–õ–ò–ß–ò–õ –¥–æ 2000 (–º–æ–∂–Ω–æ 3000, 5000)
        env = FixedPatrolEnv()
        agent = FastPatrolAgent(load_model=True)
        
        rewards = []
        coverages = []
        
        print(f"–û–±—É—á–µ–Ω–∏–µ: {episodes} —ç–ø–∏–∑–æ–¥–æ–≤...")
        start = time.time()
        
        for ep in range(episodes):
            state = env.reset()
            total = 0
            
            for step in range(env.max_steps):
                action = agent.select_action(state, training=True)
                next_state, reward, done, _ = env.step(action)
                
                agent.store(state, action, reward, next_state, done)
                loss = agent.learn()
                
                total += reward
                state = next_state
                agent.steps_done += 1
                
                if agent.steps_done % agent.update_every == 0:
                    agent.update_target()
                
                if done:
                    break
            
            agent.decay()
            rewards.append(total)
            
            coverage = np.sum(env.visit_count > 0) / 100.0
            coverages.append(coverage)
            
            if ep % 100 == 0:
                elapsed = time.time() - start
                avg_r = np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards)
                avg_c = np.mean(coverages[-100:]) if len(coverages) >= 100 else np.mean(coverages)
                print(f"–≠–ø {ep}: –ù–∞–≥—Ä–∞–¥–∞={avg_r:.2f}, "
                    f"–ü–æ–∫—Ä—ã—Ç–∏–µ={avg_c*100:.1f}%, "
                    f"Eps={agent.epsilon:.3f}, "
                    f"–í—Ä–µ–º—è={elapsed:.1f}—Å")
        
        agent.save()
        
        # –ì—Ä–∞—Ñ–∏–∫–∏
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        axes[0].plot(rewards, alpha=0.3, color='blue')
        if len(rewards) >= 30:
            ma = np.convolve(rewards, np.ones(30)/30, mode='valid')
            axes[0].plot(ma, color='red', linewidth=2)
        axes[0].set_xlabel('–≠–ø–∏–∑–æ–¥')
        axes[0].set_ylabel('–ù–∞–≥—Ä–∞–¥–∞')
        axes[0].set_title('–û–±—É—á–µ–Ω–∏–µ')
        axes[0].grid(True)
        
        axes[1].plot(coverages, alpha=0.3, color='green')
        if len(coverages) >= 30:
            ma = np.convolve(coverages, np.ones(30)/30, mode='valid')
            axes[1].plot(ma, color='darkgreen', linewidth=2)
        axes[1].set_xlabel('–≠–ø–∏–∑–æ–¥')
        axes[1].set_ylabel('–ü–æ–∫—Ä—ã—Ç–∏–µ')
        axes[1].set_title(f'–ü–æ–∫—Ä—ã—Ç–∏–µ (—Ñ–∏–Ω–∞–ª: {coverages[-1]*100:.1f}%)')
        axes[1].grid(True)
        
        plt.tight_layout()
        plt.show()
        
        return agent, env

    def demo_fixed(agent, episodes=3):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π –∫–∞—Ä—Ç–æ–π –ø–æ—Å–µ—â–µ–Ω–∏–π.
        –≠—Ç–æ 3 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ (–Ω–µ –æ–±—É—á–µ–Ω–∏–µ!).
        """
        env = FixedPatrolEnv()
        plt.ion()
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å –¥–≤—É–º—è –ø–æ–¥–≥—Ä–∞—Ñ–∏–∫–∞–º–∏: –∫–∞—Ä—Ç–∞ –∏ —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        fig = plt.figure(figsize=(14, 6))
        ax_map = plt.subplot(1, 2, 1)
        ax_heatmap = plt.subplot(1, 2, 2)
        
        # –°–æ–∑–¥–∞–µ–º colorbar –û–î–ò–ù –†–ê–ó
        im = ax_heatmap.imshow(np.zeros((10, 10)), cmap='YlOrRd', vmin=0, vmax=5)
        cbar = plt.colorbar(im, ax=ax_heatmap)
        cbar.set_label('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–µ—â–µ–Ω–∏–π')
        
        for demo_ep in range(episodes):
            state = env.reset()
            total = 0
            path = [tuple(env.pos)]
            
            for step in range(100):
                action = agent.select_action(state, training=False)
                state, reward, done, _ = env.step(action)
                total += reward
                path.append(tuple(env.pos))
                
                # –†–∏—Å—É–µ–º –∫–∞–∂–¥—ã–µ 5 —à–∞–≥–æ–≤
                if step % 5 == 0 or done:
                    # –õ–µ–≤–∞—è –ø–∞–Ω–µ–ª—å - –∫–∞—Ä—Ç–∞ –ø–∞—Ç—Ä—É–ª—è
                    ax_map.clear()
                    
                    # –°—Ç–µ–Ω—ã —á–µ—Ä–Ω—ã–µ
                    for (y, x) in env.walls:
                        ax_map.add_patch(Rectangle((x, 9-y), 1, 1, facecolor='black'))
                    
                    # –ó–æ–Ω—ã —Ä–∏—Å–∫–∞ –∫—Ä–∞—Å–Ω—ã–µ –ø–æ–ª—É–ø—Ä–æ–∑—Ä–∞—á–Ω—ã–µ
                    for (y, x) in env.risk_zones:
                        ax_map.add_patch(Rectangle((x-0.5, 9-(y-0.5)), 2, 2, 
                                                facecolor='red', alpha=0.1))
                    
                    # –ü—É—Ç—å —Å–∏–Ω–∏–π
                    if len(path) > 1:
                        xs = [p[1]+0.5 for p in path]
                        ys = [9-p[0]+0.5 for p in path]
                        ax_map.plot(xs, ys, 'b-', linewidth=2, alpha=0.7)
                    
                    # –¶–µ–ª—å –∫—Ä–∞—Å–Ω—ã–π –∫—Ä—É–≥
                    if env.targets:
                        t = env.targets[0]
                        circle = Circle((t[1]+0.5, 9-t[0]+0.5), 0.4, color='red')
                        ax_map.add_patch(circle)
                        ax_map.text(t[1]+0.5, 9-t[0]+0.5, str(t[2]), 
                                ha='center', va='center', color='white', fontsize=9)
                    
                    # –ê–≥–µ–Ω—Ç —Å–∏–Ω–∏–π –∫–≤–∞–¥—Ä–∞—Ç
                    ax_map.plot(env.pos[1]+0.5, 9-env.pos[0]+0.5, 
                            's', color='blue', markersize=20)
                    
                    ax_map.set_xlim(0, 10)
                    ax_map.set_ylim(0, 10)
                    ax_map.set_aspect('equal')
                    ax_map.set_title(f'–ü–ê–¢–†–£–õ–¨ #{demo_ep+1} (—Ç–µ—Å—Ç) | –®–∞–≥ {step} | –û—á–∫–∏: {total:.1f}')
                    
                    # –ü—Ä–∞–≤–∞—è –ø–∞–Ω–µ–ª—å - —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ—Å–µ—â–µ–Ω–∏–π (–û–ë–ù–û–í–õ–Ø–ï–ú –î–ê–ù–ù–´–ï)
                    im.set_array(env.visit_count)
                    ax_heatmap.set_title(f'–ö–∞—Ä—Ç–∞ –ø–æ—Å–µ—â–µ–Ω–∏–π\n–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–µ—Ç–æ–∫: {np.sum(env.visit_count > 0)}/100')
                    
                    plt.pause(0.3)
                
                if done:
                    break
            
            coverage = np.sum(env.visit_count > 0) / 100.0
            print(f"–ü–∞—Ç—Ä—É–ª—å {demo_ep+1}: {total:.1f} –æ—á–∫–æ–≤, –ø–æ–∫—Ä—ã—Ç–∏–µ {coverage*100:.0f}%")
            print("  (—ç—Ç–æ —Ç–µ—Å—Ç –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞, –Ω–µ –æ–±—É—á–µ–Ω–∏–µ)")
            plt.pause(1)
        
        plt.ioff()
        plt.show()

    if __name__ == "__main__":
        # –ß—Ç–æ–±—ã —É–≤–µ–ª–∏—á–∏—Ç—å —ç–ø–∏–∑–æ–¥—ã - –ø—Ä–æ—Å—Ç–æ –∏–∑–º–µ–Ω–∏ —á–∏—Å–ª–æ –∑–¥–µ—Å—å:
        # episodes=2000 (—Å–µ–π—á–∞—Å), –º–æ–∂–Ω–æ episodes=3000 –∏–ª–∏ 5000
        agent, env = train_fast(episodes=2000)
        
        # –≠—Ç–æ 3 —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—É—Å–∫–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
        print("\n=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ (3 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–∞—Ç—Ä—É–ª—è) ===")
        demo_fixed(agent, episodes=3)