import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque, namedtuple
import os
from typing import List, Tuple
from model import DuelingPatrolNet, PatrolNet


# –û–ø—ã—Ç –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ replay buffer
Experience = namedtuple(
    'Experience', ['state', 'action', 'reward', 'next_state', 'done'])


class PrioritizedReplayBuffer:
    """
    Prioritized Experience Replay Buffer.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.
    """

    def __init__(self, capacity: int = 50000, alpha: float = 0.6):
        self.capacity = capacity
        # –°—Ç–µ–ø–µ–Ω—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (0 = uniform, 1 = full priority)
        self.alpha = alpha
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0

    def push(self, state, action, reward, next_state, done):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–ø—ã—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º."""
        max_priority = self.priorities.max() if self.buffer else 1.0

        if len(self.buffer) < self.capacity:
            self.buffer.append((state, action, reward, next_state, done))
        else:
            self.buffer[self.position] = (
                state, action, reward, next_state, done)

        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size: int, beta: float = 0.4):
        """–í—ã–±–æ—Ä–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏."""
        if len(self.buffer) == 0:
            return None

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã–±–æ—Ä–∫–∏
        priorities = self.priorities[:len(self.buffer)]
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()

        # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        indices = np.random.choice(
            len(self.buffer), batch_size, p=probabilities, replace=False)

        # –í—ã—á–∏—Å–ª—è–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏ —Å–º–µ—â–µ–Ω–∏—è
        weights = (len(self.buffer) * probabilities[indices]) ** (-beta)
        weights /= weights.max()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—ã—Ç
        samples = [self.buffer[idx] for idx in indices]
        states, actions, rewards, next_states, dones = zip(*samples)

        return (
            np.array(states),
            np.array(actions),
            np.array(rewards, dtype=np.float32),
            np.array(next_states),
            np.array(dones, dtype=np.float32),
            indices,
            weights.astype(np.float32)
        )

    def update_priorities(self, indices: np.ndarray, priorities: np.ndarray):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è."""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority + \
                1e-6  # –ú–∞–ª–æ–µ —á–∏—Å–ª–æ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

    def __len__(self):
        return len(self.buffer)


class PatrolAgent:
    """
    DQN –∞–≥–µ–Ω—Ç –¥–ª—è –ø–∞—Ç—Ä—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è.
    """

    def __init__(
        self,
        map_size: int = 10,
        use_dueling: bool = True,
        load_model: bool = True,
        model_dir: str = "models"
    ):
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu")
        self.map_size = map_size
        self.model_dir = model_dir

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –º–æ–¥–µ–ª–µ–π
        os.makedirs(model_dir, exist_ok=True)
        self.model_path = os.path.join(model_dir, "patrol_model.pth")
        self.config_path = os.path.join(model_dir, "agent_config.pth")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ç–µ–π
        NetClass = DuelingPatrolNet if use_dueling else PatrolNet
        self.policy_net = NetClass(map_size).to(self.device)
        self.target_net = NetClass(map_size).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0003)
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer, step_size=500, gamma=0.9)

        # Replay buffer
        self.memory = PrioritizedReplayBuffer(capacity=50000)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.batch_size = 64
        self.gamma = 0.99
        self.tau = 0.005  # –î–ª—è soft update target —Å–µ—Ç–∏

        # Epsilon-greedy
        self.epsilon = 1.0
        self.epsilon_min = 0.05
        self.epsilon_decay = 0.995

        # –°—á–µ—Ç—á–∏–∫–∏
        self.steps_done = 0
        self.episodes_done = 0

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.training_history = {
            'losses': [],
            'rewards': [],
            'coverages': [],
            'epsilons': []
        }

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
        if load_model:
            self.load()

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """–í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è —Å epsilon-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π."""
        if training and random.random() < self.epsilon:
            return random.randint(0, 3)

        with torch.no_grad():
            state_tensor = torch.FloatTensor(
                state).unsqueeze(0).to(self.device)
            q_values = self.policy_net(state_tensor)
            return q_values.argmax(dim=1).item()

    def store_transition(self, state, action, reward, next_state, done):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ –±—É—Ñ–µ—Ä."""
        self.memory.push(state, action, reward, next_state, done)

    def learn(self) -> float:
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–µ –∏–∑ –±—É—Ñ–µ—Ä–∞."""
        if len(self.memory) < self.batch_size:
            return 0.0

        # –í—ã–±–æ—Ä–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        sample = self.memory.sample(self.batch_size, beta=0.4)
        if sample is None:
            return 0.0

        states, actions, rewards, next_states, dones, indices, weights = sample

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã
        states = torch.FloatTensor(states).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        weights = torch.FloatTensor(weights).to(self.device)

        # –¢–µ–∫—É—â–∏–µ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        current_q = self.policy_net(states).gather(
            1, actions.unsqueeze(1)).squeeze()

        # Double DQN: –∏—Å–ø–æ–ª—å–∑—É–µ–º policy_net –¥–ª—è –≤—ã–±–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–∏—è, target_net –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        with torch.no_grad():
            next_actions = self.policy_net(next_states).argmax(dim=1)
            next_q = self.target_net(next_states).gather(
                1, next_actions.unsqueeze(1)).squeeze()
            target_q = rewards + self.gamma * next_q * (1 - dones)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ —Å –≤–µ—Å–∞–º–∏
        td_errors = torch.abs(current_q - target_q).detach().cpu().numpy()
        loss = (weights * nn.functional.smooth_l1_loss(current_q,
                target_q, reduction='none')).mean()

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        self.memory.update_priorities(indices, td_errors)

        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            self.policy_net.parameters(), max_norm=1.0)
        self.optimizer.step()

        # Soft update target —Å–µ—Ç–∏
        self._soft_update_target()

        self.steps_done += 1

        return loss.item()

    def _soft_update_target(self):
        """–ú—è–≥–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ target —Å–µ—Ç–∏."""
        for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(
                self.tau * policy_param.data + (1 - self.tau) * target_param.data)

    def decay_epsilon(self):
        """–£–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon –¥–ª—è exploration."""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            self.epsilon = max(self.epsilon, self.epsilon_min)

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏
        torch.save({
            'policy_net': self.policy_net.state_dict(),
            'target_net': self.target_net.state_dict(),
            'optimizer': self.optimizer.state_dict(),
            'scheduler': self.scheduler.state_dict()
        }, self.model_path)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–≥–µ–Ω—Ç–∞
        torch.save({
            'epsilon': self.epsilon,
            'steps_done': self.steps_done,
            'episodes_done': self.episodes_done,
            'training_history': self.training_history
        }, self.config_path)

        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.model_path}")
        print(
            f"   Epsilon: {self.epsilon:.4f}, Steps: {self.steps_done}, Episodes: {self.episodes_done}")

    def load(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        if os.path.exists(self.model_path) and os.path.exists(self.config_path):
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
                checkpoint = torch.load(
                    self.model_path, map_location=self.device)
                self.policy_net.load_state_dict(checkpoint['policy_net'])
                self.target_net.load_state_dict(checkpoint['target_net'])
                self.optimizer.load_state_dict(checkpoint['optimizer'])
                self.scheduler.load_state_dict(checkpoint['scheduler'])

                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                config = torch.load(self.config_path, map_location=self.device)
                self.epsilon = config['epsilon']
                self.steps_done = config['steps_done']
                self.episodes_done = config['episodes_done']
                self.training_history = config['training_history']

                print(f"üß† –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_path}")
                print(
                    f"   Epsilon: {self.epsilon:.4f}, Steps: {self.steps_done}, Episodes: {self.episodes_done}")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
                print("   –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
                return False
        else:
            print("üÜï –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –Ω–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.")
            return False

    def get_stats(self) -> dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞."""
        return {
            'epsilon': self.epsilon,
            'steps_done': self.steps_done,
            'episodes_done': self.episodes_done,
            'memory_size': len(self.memory),
            'device': str(self.device)
        }
